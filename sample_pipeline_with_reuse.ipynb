{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading postgres module without psycopg2 installed. Will crash at runtime if postgres functionality is used.\n",
      "Loading S3 module without the python package boto3. Will crash at runtime if S3 functionality is used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to d6tflow!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import luigi\n",
    "import d6tflow\n",
    "import collections\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('data_sample_pipeline_with_reuse')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this_data_dir = 'data_sample_pipeline_with_reuse'\n",
    "if os.path.exists(this_data_dir) and os.path.isdir(this_data_dir):\n",
    "    shutil.rmtree(this_data_dir)\n",
    "d6tflow.set_dir(this_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_iterable(arg):\n",
    "    \"\"\"\n",
    "    Returns whether an argument is an iterable but not a string\n",
    "    From stackoverflow: \"how to tell a varaiable is iterable but not a string\"\n",
    "    Args:\n",
    "        arg: some variable to be tested\n",
    "    Returns:\n",
    "        (bool)\n",
    "    \"\"\"\n",
    "    return (\n",
    "            isinstance(arg, collections.Iterable)\n",
    "            and not isinstance(arg, str)\n",
    "    )\n",
    "\n",
    "\n",
    "def make_iterable(arg):\n",
    "    \"\"\"\n",
    "    Makes arg into an iterable if it isn't already (note that strings are ignored and treated as non-iterable)\n",
    "    \"\"\"\n",
    "    return arg if is_iterable(arg) else (arg,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "## Basic operation in Luigi\n",
    "\n",
    "Explain:\n",
    "* Targets and Tasks\n",
    "* requires(), run(), output() (explain how they return Target(s)), persist **not luigi right?**, param definition, subclassing\n",
    "* parameter mapping (manual, using requires/inherrits)\n",
    "\n",
    "## Naming conventions for d6tflow and luigi:\n",
    "\n",
    "### Luigi\n",
    "\n",
    "Luigi namespace nomenclature is explained [here](https://luigi.readthedocs.io/en/stable/tasks.html#namespaces-families-and-ids), but it alone feels inadequate.  It is based on\n",
    "* task_namespace: Set at the class level through class param task_namespace\n",
    "* task_family: The combination of taskNamespace.taskClassName, implicitly set at the class level by setting the namespace and defining the class\n",
    "* task_id: The combination of taskNamespace.taskClassName_\\[kwargs\\]_hashOfKwargs.  This incorporates data at the instance level, but nothing that can be specified by the user directly (only data that is set implicitly by the serialized input parameters and not by the upstream tasks)\n",
    "\n",
    "This naming convention alone does not handle cases like when you want to reuse the same task class twice.  For example:\n",
    "\n",
    "    1. Load raw file A and save as dataframe A\n",
    "        a. Drop column \"description\" from dataframe A\n",
    "    2. Load raw file A and save as dataframe B\n",
    "        a. Drop column \"description\" from dataframe B\n",
    "    3. Do work on dataframe A and B (print them, merge them, ... something that means they're in a connected pipeline)\n",
    "\n",
    "If you have a \"drop column\" Task that takes a single parameter \"column\" and you want to apply it twice in the same pipeline (say, to drop the column \"description\" from two different pieces of data in two branches of the same pipeline), if you reuse the same class for both Drop steps they will have the same task_namespace, task_family, and task_id.  \n",
    "\n",
    "This could be addressed in Luigi by defining better namespace logic in the Output() functions in the Drop Column Tasks, as is done in d6tflow.\n",
    "\n",
    "### d6tflow (layered over Luigi)\n",
    "\n",
    "d6tflow adds an additional layer on the Luigi naming conventions through it's provided abstract base Target classes, such as *d6tflow.tasks.TaskPqPandas*.  These classes implement reusable infrastructure for loading (upstream) input and saving output.  They by default can save n outputs of their given type (for example, all dt6flow ABCs implement .save(), which saves 1+ outputs (Targets) to predefined format/location.  *TaskPqPandas* implements save() to write pandas dataframes to parquet).  They also implement an interface that allows downstream tasks to load their outputs without knowing the particulars of how they're saved (a downstream task to TaskPqPandas does not need to know data is in parquet format, it just asks its upstream Task for the loaded data).  This is different from basssse Luigi, where downstream Tasks interact directly with the file objects generated by upstream Tasks and must know how to interact with them.  d6tflow does not allow for mixed outputs from a task (a Task cannot output both a df to parquet and some metadata to csv).\n",
    "\n",
    "Built into the d6tflow layer are output_dir, target_dir, and output_name conventions described below:\n",
    "\n",
    "* Task output naming conventions:\n",
    "  * (output_dir)/(target_dir)/(task_id)-(persist_output_name).extension\n",
    "  * This schema is defined in d6tflow's TaskData._getpath() (a base class inherrited by all dt6flow Tasks), which sets the output paths for all Targets (outputs) of a Task\n",
    "* output_dir: (added by d6tflow)\n",
    "  * a global output directory used by all Tasks in a pipeline in d6tflow.\n",
    "  * Set by: d6tflow.set_dir()\n",
    "* target_dir: (added by d6tflow)\n",
    "  * Subdir used by a task\n",
    "  * Set by: cls.target_dir\n",
    "* task_id: (from luigi.Task)\n",
    "  * Defined by luigi.Task.__init__() to be (task_namespace)_(task_params_as_kwargs)_(hash_of_params)\n",
    "  * Set by: indirectly (via namespace and kwargs)\n",
    "* task_namespace: (from luigi.Task)\n",
    "  * Also mentioned as a task_family in luigi code.  namespace prefix on all output files.  Feels like it argues a bit with target_dir.  Defaults to cls.__name__\n",
    "  * luigi.Task.get_task_family() and luigi.Task.get_task_namespace() are the toolchain that use this\n",
    "  * set by: cls.task_namespace\n",
    "* persist_output_name: (added by d6tflow)\n",
    "  * Naming convention used for outputs.  This defines the output naming used by the .save() function and the suffix on the output files\n",
    "  * default: cls.persist = ['data']  (single output named 'data')\n",
    "  * If only one output, the output routing functions (eg: load()) deliver it directly.  If a list, load() loads a list of the data.  (Can it handle named references using dicts too?)\n",
    "  * When .save()ing data in a Task, use a dict like: self.save({'output1': df_to_save1, 'output2': df_to_save2})\n",
    "  * set by: cls.persist\n",
    "\n",
    "## Challenges to using Luigi and d6tflow in a large workflow\n",
    "\n",
    "* Base Luigi is the wild west...\n",
    "    * Luigi provides an environment that can run tasks that depend on other tasks, but relies on the user to manage the flow of all input/output parameters and data.  For simple pipelines of custom code this is fine (TaskB can implement a requires() method that instantiates TaskA with the proper arguments and knows how to interact with the .output() of TaskA), but this requires purpose-built classes for every task\n",
    "        * This makes reusing code and large pipelines difficult.  When you create TaskZ, you need to provide it with a requires() method that calls TasyY, which is defined with a requires() method that calls TaskX(), ...  All implementations  must pipe the input parameters upstream and handle the outputs correctly.  And if we add a step with an input parameter to a pipeline, all downstream steps must be modified to pipe it correctly (Add TaskA w/param_a to the above pipeline and now TaskZ must accept param_a and pass it to TaskY, TaskY must accept it and pass it to TaskX, ...)  \n",
    "        * Luigi provides @requires() and @inherrits() decorators to automate some of this (decorating TaskZ with @requires(TaskY) will implement a basic requires() method in TaskZ that pipes parameters/receives output file handles from TaskY) but these are inadequate because they do not handle parameter name collisions (if TaskZ and TaskY both have a param called \"param_1\", @requires()/@inherrits() cannot distinguish these and they'll be treated as the same parameter).  This gets way worse when you're defining a pipeline that starts at TaskA and ends at TaskZ, or two people are independently making Tasks that happen to have parameter names that are the same  \n",
    "        * These problems are compounded by how @requires/@inherrits take Task **classes** rather than instances, so you cannot define a Task with its parameters before using it in @requires().\n",
    "* Code reuse seems challenging (or at least requires a specific workflow)\n",
    "    * Duck typing feels very hard out of the box.  If we have TaskA->TasbB, TaskB needs a requires() methods that knows exactly how to handle the Targets returned by TaskA's output().  If we now substitute TaskA' for TaskA, where TaskA' produces the same sort of product (eg: a dataframe) but saves it differently (Target returned by TaskA' points to a parquet whereas TaskA pointed to a csv), we much create a TaskB' that can handle the new TaskA' Target.  It would be better if TaskA' provided that interface\n",
    "        * This problem derives from how Luigi comes with a single (very flexible) LocalTarget Target class and the user is left to handle that.  d6tflow addresses this by implementing additional Target classes which know how to .load() themselves.  d6tflow definitely makes this easier than pure Luigi\n",
    "    * Many tasks in a pipeline are mundane and reusable (dropping columns, loading files, merging dataframes, etc.), but you cannot reuse these Task classes directly because task_namespace/task_family is defined by the class and set statically at instantiation (so modifying them on an instantce is awkward).  d6tflow's target_dir can(**?**) be set on an instance, so that makes it a bit easier to control and use\n",
    "\n",
    "## Possible workflows using Luigi and d6tflow:\n",
    "  -   Use a classmethod that subclasses and names the subclass/target_dir for you at creation to organize output data in a minimal-code way (see TaskMixin.make_subclass() below).  \n",
    "  -   have an initialization constructor rather than subclassing constructor\n",
    "      -   This CANNOT use task_namespace from luigi.Task directly as it's init() statically defines task_id based on\n",
    "          task_namespace at init() time.  task_namespace must be defined on the CLASS rather than instance because\n",
    "          it is gotten by a class methods (get_task_namespace).  I THINK target_dir would work without subclassing\n",
    "          but haven't tested fully\n",
    "      -   We could override some of these methods and then avoid needing to subclass things.\n",
    "      -   If we do a different workflow than subclassing we'd need something like:\n",
    "              my_instance = requires_decorator(upstream_task_instance)(MyClass.\\[SOMEHOW APPLY NAMESPACE/DIR AND RETURN CLS\\])(instance kwargs)\n",
    "  -   In both cases, I can instance ahead of time (for parameter setting) and use @requires_instance to add\n",
    "      requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra features added here\n",
    "(these would be put in a general package for reuse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class requires_instance(object):\n",
    "    \"\"\"\n",
    "    Modified version of luigi.util.requires, requiring instanced Tasks and ignoring inherits\n",
    "\n",
    "    TODO: Name this better!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *tasks_to_require):\n",
    "        super().__init__()\n",
    "        if not tasks_to_require:\n",
    "            raise TypeError(\"tasks_to_require cannot be empty\")\n",
    "\n",
    "        self.tasks_to_require = tasks_to_require\n",
    "\n",
    "    def __call__(self, task_that_requires):\n",
    "        # task_that_requires = inherits(*self.tasks_to_require)(task_that_requires)\n",
    "\n",
    "        # Modify task_that_requires by adding requires method.\n",
    "        # If only one task is required, this single task is returned.\n",
    "        # Otherwise, list of tasks is returned\n",
    "        def requires(_self):\n",
    "            # Should this be returning based on _self?  I'm not sure the difference...\n",
    "            # Warning about using multiple requirements with d6tflow's inputLoad().  The logic looks for\n",
    "            # a tuple, else a dict, else treats as a single Task.  For now force non-single case to a tuple, but really\n",
    "            # the logic should be changed in d6tflow\n",
    "            return self.tasks_to_require[0] if len(self.tasks_to_require) == 1 else tuple(self.tasks_to_require)\n",
    "        task_that_requires.requires = requires\n",
    "\n",
    "        return task_that_requires\n",
    "\n",
    "\n",
    "class TaskMixin:\n",
    "    @classmethod\n",
    "    def make_subclass(cls, subclass_name=None, upstream_instances=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Returns a subclass of the base class,\n",
    "\n",
    "        :param subclass_name: Name of the subclass.  Default is the parent class's name with \"BaseTask\" stripped from\n",
    "                              its ends\n",
    "        :param requires_instance: If set, adds a @requires_instance decorator on the subclass which maps the subclass's\n",
    "                                  requires using INSTANCED Tasks (not Task classes like normal Luigi)\n",
    "        :param kwargs: Anything to be overwritten on the parent class.  This is passed as the third argument of the\n",
    "                       \"type\" function and can contain class variables and functions.  Suggested use:\n",
    "                            target_dir\n",
    "                            additional class parameters?\n",
    "\n",
    "        :return:\n",
    "            The subclass (not instantiated)\n",
    "        \"\"\"\n",
    "        subclass_name = subclass_name if subclass_name else cls.__name__.strip(\"BaseTask\")\n",
    "        subclass = type(subclass_name, (cls,), kwargs)\n",
    "        if upstream_instances:\n",
    "            # Make everything a list that we can expand into the below decorator\n",
    "            upstream_instances = make_iterable(upstream_instances)\n",
    "\n",
    "            subclass = requires_instance(*upstream_instances)(subclass)\n",
    "        return subclass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples of reusable Task classes\n",
    "(these are what we'd define in a package for reusing to build a pipeline from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper for fake data...\n",
    "DATA_VALUES = [1, 2, 3]\n",
    "\n",
    "class BaseTaskGetTwoData(TaskMixin, d6tflow.tasks.TaskPqPandas):\n",
    "    \"\"\"\n",
    "    This is a fake 'data generation' task that creates two dataframes with a little data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define parameters as luigi.Parameter subclasses.  Luigi looks for these if you ever use inherrits(), and I think\n",
    "    # also checks to make sure they're defined early in a pipeline run?\n",
    "    columns = luigi.ListParameter(default=['a', 'b', 'c'])\n",
    "\n",
    "    # Set persist to explicitly name the outputs of a process and/or define more than one\n",
    "    # persist = ['data']  # Default set internally\n",
    "    persist = ['output1', 'output2']\n",
    "\n",
    "    def run(self):\n",
    "        # Fabricate some data\n",
    "        df = pd.DataFrame([{k: v*i_k for i_k, k in enumerate(self.columns)} for v in DATA_VALUES])\n",
    "\n",
    "        to_save = {output_name: df for output_name in self.persist}\n",
    "        self.save(to_save)\n",
    "\n",
    "\n",
    "class BaseTaskGetOneData(TaskMixin, d6tflow.tasks.TaskPqPandas):\n",
    "    \"\"\"\n",
    "    This is a fake 'data generation' task that creates one dataframe with a little data\n",
    "    \"\"\"\n",
    "    columns = luigi.ListParameter(default=['a', 'b', 'c'])\n",
    "\n",
    "    def run(self):\n",
    "        # Fabricate some data\n",
    "        df = pd.DataFrame([{k: v*i_k for i_k, k in enumerate(self.columns)} for v in DATA_VALUES])\n",
    "\n",
    "        self.save(df)\n",
    "\n",
    "\n",
    "class BaseTaskPrintDf(TaskMixin, d6tflow.tasks.TaskCache):\n",
    "    \"\"\"\n",
    "    This task prints one or more dataframes to the screen\n",
    "    \"\"\"\n",
    "    def run(self):\n",
    "        # Basic Luigi: .input() returns whatever self.requires() returns, so to use input() you need to interpret\n",
    "        # whatever comes from upstream (could be a Target, Tuple(Target1, ...), Tuple(Dict('name1': Target1, ...), ...)\n",
    "        # dfs = self.input().load()  # For a single input.  self.input() returns a Target, which has a .load()\n",
    "        # For loading all of multiple inputs (can also do key's here I think?).  Note it always flattens to a list.\n",
    "        # This function could be improved to reflect the structure of whatever is in requires() better\n",
    "        dfs = self.inputLoad()\n",
    "\n",
    "        # Do some \"work\"\n",
    "        print(\"Printing results:\")\n",
    "        for i, df in enumerate(dfs):\n",
    "            print(f\"df {i}\")\n",
    "            print(df)\n",
    "\n",
    "\n",
    "class BaseTaskDropColumn(TaskMixin, d6tflow.tasks.TaskPqPandas):\n",
    "    \"\"\"\n",
    "    Drop a column from a dataframe, saving the resulting dataframe for future tasks\n",
    "    \"\"\"\n",
    "    column = luigi.Parameter()\n",
    "\n",
    "    def run(self):\n",
    "        df = self.input().load()\n",
    "\n",
    "        df = df.drop(columns=[self.column])\n",
    "        self.save(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/schematic_of_pipeline.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scribs/code_git_backed/d6tflow-recipes/venv/lib/python3.7/site-packages/ipykernel_launcher.py:11: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# Example of defining output by making a Subclass and then instantiating it\n",
    "# Subclassing gives us control over the target_dir.  Everything from the same subclass goes to the same target_dir, so\n",
    "# each Task (atomic action in a pipeline) needs its own subclass of a Task\n",
    "TaskGetTwoData = BaseTaskGetTwoData.make_subclass(subclass_name=\"MyGetTwoData\", target_dir='my_get_two_data')\n",
    "my_get_two_data = TaskGetTwoData(columns=['a', 'b', 'c'])\n",
    "\n",
    "# We can also do it all in one command\n",
    "my_get_two_data2 = BaseTaskGetTwoData.make_subclass(subclass_name=\"MyGetTwoData2\",\n",
    "                                                    target_dir='my_get_two_data2',)(columns=['d', 'e', 'f'])\n",
    "\n",
    "my_get_one_data = BaseTaskGetOneData.make_subclass(subclass_name=\"MyGetOneData\",\n",
    "                                                   target_dir='my_get_one_data')(columns=['aa', 'bb'])\n",
    "\n",
    "# Downstream tasks can consume upstream, fully instantiated tasks\n",
    "my_drop_column = BaseTaskDropColumn.make_subclass(subclass_name=\"MyDropColumn\", \n",
    "                                                  target_dir=\"my_drop_column\", \n",
    "                                                  upstream_instances=[my_get_one_data])(column='aa')\n",
    "\n",
    "my_print_df_1 = BaseTaskPrintDf.make_subclass(subclass_name=\"MyPrintDf1\", upstream_instances=my_drop_column)()\n",
    "\n",
    "# We can also map multiple inputs to a single task\n",
    "# (target_dir doesn't matter here - no output!)\n",
    "my_print_df_2 = BaseTaskPrintDf.make_subclass(upstream_instances=[\n",
    "                                                                  my_get_two_data,\n",
    "                                                                  my_get_two_data2,\n",
    "                                                                  my_drop_column,\n",
    "                                                                  ],\n",
    "                                              target_dir='this_doesnt_matter_but')()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This runs everything it needs because nothing is pre-computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Informed scheduler that task   MyPrintDf1__99914b932b   has status   PENDING\n",
      "INFO: Informed scheduler that task   MyDropColumn_aa_8d8cf8322c   has status   PENDING\n",
      "INFO: Informed scheduler that task   MyGetOneData___aa____bb___24e1609319   has status   PENDING\n",
      "INFO: Done scheduling tasks\n",
      "INFO: Running Worker with 1 processes\n",
      "INFO: [pid 22899] Worker Worker(salt=853514804, workers=1, host=scribs-desktop, username=scribs, pid=22899) running   MyGetOneData(columns=[\"aa\", \"bb\"])\n",
      "INFO: [pid 22899] Worker Worker(salt=853514804, workers=1, host=scribs-desktop, username=scribs, pid=22899) done      MyGetOneData(columns=[\"aa\", \"bb\"])\n",
      "INFO: Informed scheduler that task   MyGetOneData___aa____bb___24e1609319   has status   DONE\n",
      "INFO: [pid 22899] Worker Worker(salt=853514804, workers=1, host=scribs-desktop, username=scribs, pid=22899) running   MyDropColumn(column=aa)\n",
      "INFO: [pid 22899] Worker Worker(salt=853514804, workers=1, host=scribs-desktop, username=scribs, pid=22899) done      MyDropColumn(column=aa)\n",
      "INFO: Informed scheduler that task   MyDropColumn_aa_8d8cf8322c   has status   DONE\n",
      "INFO: [pid 22899] Worker Worker(salt=853514804, workers=1, host=scribs-desktop, username=scribs, pid=22899) running   MyPrintDf1()\n",
      "INFO: [pid 22899] Worker Worker(salt=853514804, workers=1, host=scribs-desktop, username=scribs, pid=22899) done      MyPrintDf1()\n",
      "INFO: Informed scheduler that task   MyPrintDf1__99914b932b   has status   DONE\n",
      "INFO: Worker Worker(salt=853514804, workers=1, host=scribs-desktop, username=scribs, pid=22899) was stopped. Shutting down Keep-Alive thread\n",
      "INFO: \n",
      "===== Luigi Execution Summary =====\n",
      "\n",
      "Scheduled 3 tasks of which:\n",
      "* 3 ran successfully:\n",
      "    - 1 MyDropColumn(column=aa)\n",
      "    - 1 MyGetOneData(columns=[\"aa\", \"bb\"])\n",
      "    - 1 MyPrintDf1()\n",
      "\n",
      "This progress looks :) because there were no failed tasks or missing dependencies\n",
      "\n",
      "===== Luigi Execution Summary =====\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing results:\n",
      "df 0\n",
      "bb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .run() is supposed to support multiple tasks but it wasn't working.  We run once for each (could also have a single\n",
    "# aggregation task that requires all the above tasks)\n",
    "d6tflow.run(my_print_df_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This only runs what didn't run above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Informed scheduler that task   PrintDf__99914b932b   has status   PENDING\n",
      "INFO: Informed scheduler that task   MyDropColumn_aa_8d8cf8322c   has status   DONE\n",
      "INFO: Informed scheduler that task   MyGetTwoData2___d____e____f___a0fcbe71f2   has status   PENDING\n",
      "INFO: Informed scheduler that task   MyGetTwoData___a____b____c___bc7315d43a   has status   PENDING\n",
      "INFO: Done scheduling tasks\n",
      "INFO: Running Worker with 1 processes\n",
      "INFO: [pid 22899] Worker Worker(salt=076408501, workers=1, host=scribs-desktop, username=scribs, pid=22899) running   MyGetTwoData2(columns=[\"d\", \"e\", \"f\"])\n",
      "INFO: [pid 22899] Worker Worker(salt=076408501, workers=1, host=scribs-desktop, username=scribs, pid=22899) done      MyGetTwoData2(columns=[\"d\", \"e\", \"f\"])\n",
      "INFO: Informed scheduler that task   MyGetTwoData2___d____e____f___a0fcbe71f2   has status   DONE\n",
      "INFO: [pid 22899] Worker Worker(salt=076408501, workers=1, host=scribs-desktop, username=scribs, pid=22899) running   MyGetTwoData(columns=[\"a\", \"b\", \"c\"])\n",
      "INFO: [pid 22899] Worker Worker(salt=076408501, workers=1, host=scribs-desktop, username=scribs, pid=22899) done      MyGetTwoData(columns=[\"a\", \"b\", \"c\"])\n",
      "INFO: Informed scheduler that task   MyGetTwoData___a____b____c___bc7315d43a   has status   DONE\n",
      "INFO: [pid 22899] Worker Worker(salt=076408501, workers=1, host=scribs-desktop, username=scribs, pid=22899) running   PrintDf()\n",
      "INFO: [pid 22899] Worker Worker(salt=076408501, workers=1, host=scribs-desktop, username=scribs, pid=22899) done      PrintDf()\n",
      "INFO: Informed scheduler that task   PrintDf__99914b932b   has status   DONE\n",
      "INFO: Worker Worker(salt=076408501, workers=1, host=scribs-desktop, username=scribs, pid=22899) was stopped. Shutting down Keep-Alive thread\n",
      "INFO: \n",
      "===== Luigi Execution Summary =====\n",
      "\n",
      "Scheduled 4 tasks of which:\n",
      "* 1 complete ones were encountered:\n",
      "    - 1 MyDropColumn(column=aa)\n",
      "* 3 ran successfully:\n",
      "    - 1 MyGetTwoData(columns=[\"a\", \"b\", \"c\"])\n",
      "    - 1 MyGetTwoData2(columns=[\"d\", \"e\", \"f\"])\n",
      "    - 1 PrintDf()\n",
      "\n",
      "This progress looks :) because there were no failed tasks or missing dependencies\n",
      "\n",
      "===== Luigi Execution Summary =====\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing results:\n",
      "df 0\n",
      "   a  b  c\n",
      "0  0  1  2\n",
      "1  0  2  4\n",
      "2  0  3  6\n",
      "df 1\n",
      "   a  b  c\n",
      "0  0  1  2\n",
      "1  0  2  4\n",
      "2  0  3  6\n",
      "df 2\n",
      "   d  e  f\n",
      "0  0  1  2\n",
      "1  0  2  4\n",
      "2  0  3  6\n",
      "df 3\n",
      "   d  e  f\n",
      "0  0  1  2\n",
      "1  0  2  4\n",
      "2  0  3  6\n",
      "df 4\n",
      "bb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d6tflow.run(my_print_df_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see the results for these cached in the data directory\n",
    "\n",
    "There is a directory for each pipeline step (defined by target_dir) and the filenames depend on the schema described in notes above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_sample_pipeline_with_reuse:\n",
      "total 16\n",
      "drwxr-xr-x 2 scribs scribs 4096 Apr 30 17:33 my_drop_column\n",
      "drwxr-xr-x 2 scribs scribs 4096 Apr 30 17:33 my_get_one_data\n",
      "drwxr-xr-x 2 scribs scribs 4096 Apr 30 17:33 my_get_two_data\n",
      "drwxr-xr-x 2 scribs scribs 4096 Apr 30 17:33 my_get_two_data2\n",
      "\n",
      "data_sample_pipeline_with_reuse/my_drop_column:\n",
      "total 4\n",
      "-rw-r--r-- 1 scribs scribs 1685 Apr 30 17:33 MyDropColumn_aa_8d8cf8322c-data.parquet\n",
      "\n",
      "data_sample_pipeline_with_reuse/my_get_one_data:\n",
      "total 4\n",
      "-rw-r--r-- 1 scribs scribs 2317 Apr 30 17:33 MyGetOneData___aa____bb___24e1609319-data.parquet\n",
      "\n",
      "data_sample_pipeline_with_reuse/my_get_two_data:\n",
      "total 8\n",
      "-rw-r--r-- 1 scribs scribs 2928 Apr 30 17:33 MyGetTwoData___a____b____c___bc7315d43a-output1.parquet\n",
      "-rw-r--r-- 1 scribs scribs 2928 Apr 30 17:33 MyGetTwoData___a____b____c___bc7315d43a-output2.parquet\n",
      "\n",
      "data_sample_pipeline_with_reuse/my_get_two_data2:\n",
      "total 8\n",
      "-rw-r--r-- 1 scribs scribs 2928 Apr 30 17:33 MyGetTwoData2___d____e____f___a0fcbe71f2-output1.parquet\n",
      "-rw-r--r-- 1 scribs scribs 2928 Apr 30 17:33 MyGetTwoData2___d____e____f___a0fcbe71f2-output2.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -lR $this_data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And reusing parts (or all) of the above pipeline.  We see here they don't run a second time (except the prints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Informed scheduler that task   MyPrintDf1__99914b932b   has status   PENDING\n",
      "INFO: Informed scheduler that task   MyDropColumn_aa_8d8cf8322c   has status   DONE\n",
      "INFO: Done scheduling tasks\n",
      "INFO: Running Worker with 1 processes\n",
      "INFO: [pid 22899] Worker Worker(salt=722917280, workers=1, host=scribs-desktop, username=scribs, pid=22899) running   MyPrintDf1()\n",
      "INFO: [pid 22899] Worker Worker(salt=722917280, workers=1, host=scribs-desktop, username=scribs, pid=22899) done      MyPrintDf1()\n",
      "INFO: Informed scheduler that task   MyPrintDf1__99914b932b   has status   DONE\n",
      "INFO: Worker Worker(salt=722917280, workers=1, host=scribs-desktop, username=scribs, pid=22899) was stopped. Shutting down Keep-Alive thread\n",
      "INFO: \n",
      "===== Luigi Execution Summary =====\n",
      "\n",
      "Scheduled 2 tasks of which:\n",
      "* 1 complete ones were encountered:\n",
      "    - 1 MyDropColumn(column=aa)\n",
      "* 1 ran successfully:\n",
      "    - 1 MyPrintDf1()\n",
      "\n",
      "This progress looks :) because there were no failed tasks or missing dependencies\n",
      "\n",
      "===== Luigi Execution Summary =====\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing results:\n",
      "df 0\n",
      "bb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d6tflow.run(my_print_df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Informed scheduler that task   PrintDf__99914b932b   has status   PENDING\n",
      "INFO: Informed scheduler that task   MyDropColumn_aa_8d8cf8322c   has status   DONE\n",
      "INFO: Informed scheduler that task   MyGetTwoData2___d____e____f___a0fcbe71f2   has status   DONE\n",
      "INFO: Informed scheduler that task   MyGetTwoData___a____b____c___bc7315d43a   has status   DONE\n",
      "INFO: Done scheduling tasks\n",
      "INFO: Running Worker with 1 processes\n",
      "INFO: [pid 22899] Worker Worker(salt=088989659, workers=1, host=scribs-desktop, username=scribs, pid=22899) running   PrintDf()\n",
      "INFO: [pid 22899] Worker Worker(salt=088989659, workers=1, host=scribs-desktop, username=scribs, pid=22899) done      PrintDf()\n",
      "INFO: Informed scheduler that task   PrintDf__99914b932b   has status   DONE\n",
      "INFO: Worker Worker(salt=088989659, workers=1, host=scribs-desktop, username=scribs, pid=22899) was stopped. Shutting down Keep-Alive thread\n",
      "INFO: \n",
      "===== Luigi Execution Summary =====\n",
      "\n",
      "Scheduled 4 tasks of which:\n",
      "* 3 complete ones were encountered:\n",
      "    - 1 MyDropColumn(column=aa)\n",
      "    - 1 MyGetTwoData(columns=[\"a\", \"b\", \"c\"])\n",
      "    - 1 MyGetTwoData2(columns=[\"d\", \"e\", \"f\"])\n",
      "* 1 ran successfully:\n",
      "    - 1 PrintDf()\n",
      "\n",
      "This progress looks :) because there were no failed tasks or missing dependencies\n",
      "\n",
      "===== Luigi Execution Summary =====\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing results:\n",
      "df 0\n",
      "   a  b  c\n",
      "0  0  1  2\n",
      "1  0  2  4\n",
      "2  0  3  6\n",
      "df 1\n",
      "   a  b  c\n",
      "0  0  1  2\n",
      "1  0  2  4\n",
      "2  0  3  6\n",
      "df 2\n",
      "   d  e  f\n",
      "0  0  1  2\n",
      "1  0  2  4\n",
      "2  0  3  6\n",
      "df 3\n",
      "   d  e  f\n",
      "0  0  1  2\n",
      "1  0  2  4\n",
      "2  0  3  6\n",
      "df 4\n",
      "bb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d6tflow.run(my_print_df_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterating on a pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can invalidate part of a pipeline, then invoke the task again and only some code reruns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compeleted tasks to invalidate:\n",
      "[MyDropColumn(column=aa), MyGetOneData(columns=[\"aa\", \"bb\"])]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Confirm invalidating tasks (y/n) n\n"
     ]
    }
   ],
   "source": [
    "d6tflow.invalidate_upstream(my_drop_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "invalidate_downstream() missing 1 required positional argument: 'task_downstream'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-672e6761383d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md6tflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvalidate_downstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_drop_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: invalidate_downstream() missing 1 required positional argument: 'task_downstream'"
     ]
    }
   ],
   "source": [
    "d6tflow.invalidate_downstream(my_drop_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "taskflow_downstream() missing 1 required positional argument: 'task_downstream'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-e80a4c46a5cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md6tflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtaskflow_downstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_drop_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: taskflow_downstream() missing 1 required positional argument: 'task_downstream'"
     ]
    }
   ],
   "source": [
    "d6tflow.taskflow_downstream(my_drop_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invalidate only this task, this plus downstream, this plus upstream, or all\n",
    "# my_get_one_data.invalidate(confirm=False)\n",
    "d6tflow.invalidate_downstream(my_get_one_data)\n",
    "d6tflow.invalidate_upstream(my_get_one_data)\n",
    "d6tflow.invalidate_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we see that the data for my_get_one_data is deleted, as is my_drop_column\n",
    "**NOTE: my_drop_column wasn't deleted but it still says it reran below.  Need to investigate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_sample_pipeline_with_reuse:\n",
      "total 16\n",
      "drwxr-xr-x 2 scribs scribs 4096 Apr 30 17:33 my_drop_column\n",
      "drwxr-xr-x 2 scribs scribs 4096 Apr 30 17:33 my_get_one_data\n",
      "drwxr-xr-x 2 scribs scribs 4096 Apr 30 17:33 my_get_two_data\n",
      "drwxr-xr-x 2 scribs scribs 4096 Apr 30 17:33 my_get_two_data2\n",
      "\n",
      "data_sample_pipeline_with_reuse/my_drop_column:\n",
      "total 4\n",
      "-rw-r--r-- 1 scribs scribs 1685 Apr 30 17:33 MyDropColumn_aa_8d8cf8322c-data.parquet\n",
      "\n",
      "data_sample_pipeline_with_reuse/my_get_one_data:\n",
      "total 0\n",
      "\n",
      "data_sample_pipeline_with_reuse/my_get_two_data:\n",
      "total 8\n",
      "-rw-r--r-- 1 scribs scribs 2928 Apr 30 17:33 MyGetTwoData___a____b____c___bc7315d43a-output1.parquet\n",
      "-rw-r--r-- 1 scribs scribs 2928 Apr 30 17:33 MyGetTwoData___a____b____c___bc7315d43a-output2.parquet\n",
      "\n",
      "data_sample_pipeline_with_reuse/my_get_two_data2:\n",
      "total 8\n",
      "-rw-r--r-- 1 scribs scribs 2928 Apr 30 17:33 MyGetTwoData2___d____e____f___a0fcbe71f2-output1.parquet\n",
      "-rw-r--r-- 1 scribs scribs 2928 Apr 30 17:33 MyGetTwoData2___d____e____f___a0fcbe71f2-output2.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -lR $this_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Informed scheduler that task   PrintDf__99914b932b   has status   PENDING\n",
      "INFO: Informed scheduler that task   MyDropColumn_aa_8d8cf8322c   has status   PENDING\n",
      "INFO: Informed scheduler that task   MyGetOneData___aa____bb___24e1609319   has status   PENDING\n",
      "INFO: Informed scheduler that task   MyGetTwoData2___d____e____f___a0fcbe71f2   has status   DONE\n",
      "INFO: Informed scheduler that task   MyGetTwoData___a____b____c___bc7315d43a   has status   DONE\n",
      "INFO: Done scheduling tasks\n",
      "INFO: Running Worker with 1 processes\n",
      "INFO: [pid 22899] Worker Worker(salt=339414248, workers=1, host=scribs-desktop, username=scribs, pid=22899) running   MyGetOneData(columns=[\"aa\", \"bb\"])\n",
      "INFO: [pid 22899] Worker Worker(salt=339414248, workers=1, host=scribs-desktop, username=scribs, pid=22899) done      MyGetOneData(columns=[\"aa\", \"bb\"])\n",
      "INFO: Informed scheduler that task   MyGetOneData___aa____bb___24e1609319   has status   DONE\n",
      "INFO: [pid 22899] Worker Worker(salt=339414248, workers=1, host=scribs-desktop, username=scribs, pid=22899) running   MyDropColumn(column=aa)\n",
      "INFO: [pid 22899] Worker Worker(salt=339414248, workers=1, host=scribs-desktop, username=scribs, pid=22899) done      MyDropColumn(column=aa)\n",
      "INFO: Informed scheduler that task   MyDropColumn_aa_8d8cf8322c   has status   DONE\n",
      "INFO: [pid 22899] Worker Worker(salt=339414248, workers=1, host=scribs-desktop, username=scribs, pid=22899) running   PrintDf()\n",
      "INFO: [pid 22899] Worker Worker(salt=339414248, workers=1, host=scribs-desktop, username=scribs, pid=22899) done      PrintDf()\n",
      "INFO: Informed scheduler that task   PrintDf__99914b932b   has status   DONE\n",
      "INFO: Worker Worker(salt=339414248, workers=1, host=scribs-desktop, username=scribs, pid=22899) was stopped. Shutting down Keep-Alive thread\n",
      "INFO: \n",
      "===== Luigi Execution Summary =====\n",
      "\n",
      "Scheduled 5 tasks of which:\n",
      "* 2 complete ones were encountered:\n",
      "    - 1 MyGetTwoData(columns=[\"a\", \"b\", \"c\"])\n",
      "    - 1 MyGetTwoData2(columns=[\"d\", \"e\", \"f\"])\n",
      "* 3 ran successfully:\n",
      "    - 1 MyDropColumn(column=aa)\n",
      "    - 1 MyGetOneData(columns=[\"aa\", \"bb\"])\n",
      "    - 1 PrintDf()\n",
      "\n",
      "This progress looks :) because there were no failed tasks or missing dependencies\n",
      "\n",
      "===== Luigi Execution Summary =====\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing results:\n",
      "df 0\n",
      "   a  b  c\n",
      "0  0  1  2\n",
      "1  0  2  4\n",
      "2  0  3  6\n",
      "df 1\n",
      "   a  b  c\n",
      "0  0  1  2\n",
      "1  0  2  4\n",
      "2  0  3  6\n",
      "df 2\n",
      "   d  e  f\n",
      "0  0  1  2\n",
      "1  0  2  4\n",
      "2  0  3  6\n",
      "df 3\n",
      "   d  e  f\n",
      "0  0  1  2\n",
      "1  0  2  4\n",
      "2  0  3  6\n",
      "df 4\n",
      "bb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d6tflow.run(my_print_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_sample_pipeline_with_reuse:\n",
      "total 16\n",
      "drwxr-xr-x 2 scribs scribs 4096 Apr 30 17:33 my_drop_column\n",
      "drwxr-xr-x 2 scribs scribs 4096 Apr 30 17:33 my_get_one_data\n",
      "drwxr-xr-x 2 scribs scribs 4096 Apr 30 17:33 my_get_two_data\n",
      "drwxr-xr-x 2 scribs scribs 4096 Apr 30 17:33 my_get_two_data2\n",
      "\n",
      "data_sample_pipeline_with_reuse/my_drop_column:\n",
      "total 4\n",
      "-rw-r--r-- 1 scribs scribs 1685 Apr 30 17:33 MyDropColumn_aa_8d8cf8322c-data.parquet\n",
      "\n",
      "data_sample_pipeline_with_reuse/my_get_one_data:\n",
      "total 4\n",
      "-rw-r--r-- 1 scribs scribs 2317 Apr 30 17:33 MyGetOneData___aa____bb___24e1609319-data.parquet\n",
      "\n",
      "data_sample_pipeline_with_reuse/my_get_two_data:\n",
      "total 8\n",
      "-rw-r--r-- 1 scribs scribs 2928 Apr 30 17:33 MyGetTwoData___a____b____c___bc7315d43a-output1.parquet\n",
      "-rw-r--r-- 1 scribs scribs 2928 Apr 30 17:33 MyGetTwoData___a____b____c___bc7315d43a-output2.parquet\n",
      "\n",
      "data_sample_pipeline_with_reuse/my_get_two_data2:\n",
      "total 8\n",
      "-rw-r--r-- 1 scribs scribs 2928 Apr 30 17:33 MyGetTwoData2___d____e____f___a0fcbe71f2-output1.parquet\n",
      "-rw-r--r-- 1 scribs scribs 2928 Apr 30 17:33 MyGetTwoData2___d____e____f___a0fcbe71f2-output2.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -lR $this_data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now they've rerun, but the others didn't have to!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d6tflow-recipes-using-fork",
   "language": "python",
   "name": "d6tflow-recipes-using-fork"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
